{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from copy import deepcopy\n",
    "\n",
    "from src.data import *\n",
    "from src.model import *\n",
    "from src.recourse import *\n",
    "from src.utils import *\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_result(d, alg_name, seed, alpha, lamb, i, x_0, theta_0, beta, x_r, theta_r, J):\n",
    "    d['alg'].append(alg_name)\n",
    "    d['seed'].append(seed)\n",
    "    d['alpha'].append(alpha)\n",
    "    d['lambda'].append(lamb)\n",
    "    d['i'].append(i)\n",
    "    d['x_0'].append(x_0.round(4))\n",
    "    d['theta_0'].append(theta_0.round(4))\n",
    "    d['beta'].append(beta)\n",
    "    d['x_r'].append(x_r.round(4))\n",
    "    d['theta_r'].append(theta_r.round(4))\n",
    "    d['J'].append(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recourse_runner(seed: int, X_train: np.ndarray, X: np.ndarray, roar_recourse: ROAR, base_model: NN, params: dict, dataset: Dataset):\n",
    "    alpha = params['alpha']\n",
    "    lamb = params['lamb']\n",
    "    weights_0, bias_0 = roar_recourse.weights.numpy(), roar_recourse.bias.numpy()\n",
    "    theta_0 = np.hstack((weights_0, bias_0))\n",
    "    \n",
    "    results = {'alg': [], 'seed': [], 'alpha': [], 'lambda': [], 'i': [], 'x_0': [], 'theta_0': [], 'beta': [], 'x_r': [], 'theta_r': [], 'J': []}\n",
    "    \n",
    "    n = len(X)\n",
    "    for i in tqdm.trange(n, desc=f'Evaluating recourse | alpha={alpha}; lambda={lamb}', colour='#0091ff'):\n",
    "        x_0 = X[i]\n",
    "        J = RecourseCost(x_0, lamb)\n",
    "        \n",
    "        if params['base_model'] == 'nn':\n",
    "            # LIME approximation of original NN\n",
    "            np.random.seed(i)\n",
    "            weights_0, bias_0 = lime_explanation(base_model.predict, X_train, x_0)\n",
    "            weights_0, bias_0 = np.round(weights_0, 4), np.round(bias_0, 4)\n",
    "            theta_0 = np.hstack((weights_0, bias_0))\n",
    "            \n",
    "            # Initalize recourse methods with theta_0\n",
    "            roar_recourse.set_weights(weights_0)\n",
    "            roar_recourse.set_bias(bias_0)\n",
    "        \n",
    "        beta = 1.\n",
    "        # ROAR with L-inf Norm Adversary\n",
    "        x_r, _ = roar_recourse.get_recourse(x_0, beta=beta, w_norm='L-inf')\n",
    "        weights_r, bias_r = roar_recourse.calc_theta_adv(x_r)\n",
    "        theta_r = np.hstack((weights_r, bias_r))\n",
    "        J_r = J.eval(x_r, weights_r, bias_r)\n",
    "        append_result(results, 'ROAR L-inf', seed, alpha, lamb, i, x_0, theta_0, beta, x_r, theta_r, J_r[0])\n",
    "        \n",
    "        # ROAR with L-1 Norm Adversary\n",
    "        x_r,_ = roar_recourse.get_recourse(x_0, beta=beta, w_norm='L-1')\n",
    "        weights_r, bias_r = roar_recourse.calc_theta_adv(x_r)\n",
    "        theta_r = np.hstack((weights_r, bias_r))\n",
    "        J_r = J.eval(x_r, weights_r, bias_r)\n",
    "        append_result(results, 'ROAR L-1', seed, alpha, lamb, i, x_0, theta_0, beta, x_r, theta_r, J_r[0])\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset: Dataset, params: dict, results: List):\n",
    "    alpha = params['alpha']\n",
    "    \n",
    "    for seed in params['seeds']:\n",
    "        (train_data, test_data) = dataset.get_data(seed)\n",
    "        X_train, y_train = train_data\n",
    "        X_test, y_test = test_data\n",
    "        \n",
    "        weights, bias = None, None\n",
    "        if params['base_model'] == nn:\n",
    "            base_model = NN(X_train.shape[1])\n",
    "            base_model.train(X_train.values, y_train.values)\n",
    "        else:\n",
    "            base_model = LR()\n",
    "            base_model.train(X_train.values, y_train.values)\n",
    "            weights = base_model.model.coef_[0]\n",
    "            bias = base_model.model.intercept_\n",
    "        \n",
    "        recourse_needed_X_train = recourse_needed(base_model.predict, X_train.values)\n",
    "        recourse_needed_X_test = recourse_needed(base_model.predict, X_test.values)\n",
    "        \n",
    "        lar_recourse = LARRecourse(weights=weights, bias=bias, alpha=alpha)\n",
    "        roar_recourse = ROAR(weights=weights, bias=bias, alpha=alpha)\n",
    "        \n",
    "        params['lamb'] = lar_recourse.choose_lambda(recourse_needed_X_train, base_model.predict, X_train.values)\n",
    "        lar_recourse.lamb = params['lamb']\n",
    "        roar_recourse.lamb = params['lamb']\n",
    "        \n",
    "        df_results = recourse_runner(seed, X_train.values, recourse_needed_X_test[:], roar_recourse, base_model, params, dataset)\n",
    "        results.append(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sba data...\n",
      "Choosing lambda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lambda=0.1: 100%|██████████| 150/150 [00:00<00:00, 29030.34it/s]\n",
      "lambda=0.2: 100%|██████████| 150/150 [00:00<00:00, 26288.89it/s]\n",
      "lambda=0.3: 100%|██████████| 150/150 [00:00<00:00, 24546.28it/s]\n",
      "lambda=0.4: 100%|██████████| 150/150 [00:00<00:00, 28100.66it/s]\n",
      "lambda=0.5: 100%|██████████| 150/150 [00:00<00:00, 26891.16it/s]\n",
      "lambda=0.6: 100%|██████████| 150/150 [00:00<00:00, 27994.38it/s]\n",
      "lambda=0.7: 100%|██████████| 150/150 [00:00<00:00, 27456.82it/s]\n",
      "lambda=0.8: 100%|██████████| 150/150 [00:00<00:00, 26886.56it/s]\n",
      "lambda=0.9: 100%|██████████| 150/150 [00:00<00:00, 27726.66it/s]\n",
      "lambda=1.0: 100%|██████████| 150/150 [00:00<00:00, 27323.27it/s]\n",
      "Evaluating recourse | alpha=0.5; lambda=1.0: 100%|\u001b[38;2;0;145;255m██████████\u001b[0m| 39/39 [00:39<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished sba\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "d_results = {}\n",
    "params = {}\n",
    "params['alpha'] = 0.5\n",
    "params['lamb'] = None\n",
    "params['seeds'] = range(1)\n",
    "params['base_model'] = 'nn'\n",
    "\n",
    "datasets = [SBADataset()]\n",
    "for dataset in datasets:\n",
    "    results = []\n",
    "    \n",
    "    print(f'Running {dataset.name} data...')\n",
    "    run_experiment(dataset, params, results)\n",
    "    d_results[dataset.name] = pd.concat(results)\n",
    "    print(f'Finished {dataset.name}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBA | NN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alg</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ROAR L-1</th>\n",
       "      <td>1.961248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROAR L-inf</th>\n",
       "      <td>4.916656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   J\n",
       "alg                 \n",
       "ROAR L-1    1.961248\n",
       "ROAR L-inf  4.916656"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f'{dataset.name.upper()} | {params[\"base_model\"].upper()}')\n",
    "    df_result = d_results[dataset.name]\n",
    "    df_agg = df_result.groupby(['alg']).mean(True)[['J']]\n",
    "    display(df_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l1 = df_result[df_result['alg']=='ROAR L-1']\n",
    "df_linf = df_result[df_result['alg']=='ROAR L-inf']\n",
    "for i in range(len(df_l1)):\n",
    "    theta_0 = df_l1['theta_0'].iloc[i]\n",
    "    theta_r = df_l1['theta_r'].iloc[i]\n",
    "    assert(np.linalg.norm(theta_0-theta_r, 1).round(2) <= params['alpha'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
