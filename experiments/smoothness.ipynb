{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from typing import List, Callable\n",
    "\n",
    "from src.data import *\n",
    "from src.model import *\n",
    "from src.utils import *\n",
    "from src.recourse import *\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_result(d, alg_name, seed, alpha, lamb, beta, p, prediction, j_val, cost):\n",
    "    d['alg'].append(alg_name)\n",
    "    d['seed'].append(seed)\n",
    "    d['alpha'].append(alpha)\n",
    "    d['lambda'].append(lamb)\n",
    "    d['p'].append(p)\n",
    "    d['prediction'].append(prediction)\n",
    "    d['beta'].append(beta)\n",
    "    d['J'].append(j_val)\n",
    "    d['Cost'].append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recourse_runner(seed:int, X: np.ndarray, lar_recourse: LARRecourse, lar_recourse_shifted: LARRecourse, roar_recourse: ROAR, base_model: Model, shifted_model: Model, X_train: np.ndarray, params: dict, dataset: Dataset, predictions: List):\n",
    "    alpha = params['alpha']\n",
    "    lamb = params['lamb']\n",
    "    betas = np.arange(0., 1.01, 0.01).round(2)\n",
    "    \n",
    "    if isinstance(base_model, LR):\n",
    "        weights_s = shifted_model.model.coef_[0]\n",
    "        bias_s = shifted_model.model.intercept_\n",
    "        \n",
    "    results_opt = {'alg': [], 'seed': [], 'alpha': [], 'lambda': [], 'p': [], 'prediction': [], 'beta': [], 'Cost': [], 'J': []}\n",
    "    results_roar = deepcopy(results_opt)\n",
    "    \n",
    "    n = len(X)\n",
    "    for i in tqdm.trange(n, desc=f'Evaluating recourse | alpha={alpha}; lambda={lamb}', colour='#0091ff'):\n",
    "        x_0 = X[i]\n",
    "        J = RecourseCost(x_0, lamb)\n",
    "        \n",
    "        if isinstance(base_model, NN):\n",
    "            weights_0, bias_0 = lime_explanation(base_model.predict, X_train, x_0)\n",
    "            weights_0, bias_0 = np.round(weights_0, 4), np.round(bias_0, 4)\n",
    "            theta_0 = np.hstack((weights_0, bias_0))\n",
    "        \n",
    "            lar_recourse.weights = weights_0\n",
    "            lar_recourse.bias = bias_0\n",
    "            \n",
    "            roar_recourse.set_weights(weights_0)\n",
    "            roar_recourse.set_bias(bias_0)\n",
    "            \n",
    "            weights_s, bias_s = lime_explanation(shifted_model.predict, X_train, x_0)\n",
    "            weights_s, bias_s = np.round(weights_0, 4), np.round(bias_0, 4)\n",
    "            theta_s = np.hstack((weights_s, bias_s))\n",
    "            lar_recourse_shifted.weights = weights_s\n",
    "            lar_recourse_shifted.bias = bias_s\n",
    "            \n",
    "            predictions = generate_nn_smoothness_predictions(theta_0, theta_s, alpha)\n",
    "            \n",
    "        x_s = lar_recourse_shifted.get_recourse(x_0, beta=0., theta_p=(weights_s, bias_s))\n",
    "        \n",
    "        # Compute J value of x_s\n",
    "        J_opt_shifted = J.eval(x_s, weights_s, bias_s)\n",
    "        \n",
    "        for p, theta_p in predictions:\n",
    "            for beta in betas:\n",
    "                if 'alg1' in params['algs']:\n",
    "                    x = lar_recourse.get_recourse(x_0, beta=beta, theta_p=theta_p)\n",
    "                    J_val = J.eval(x, weights_s, bias_s)\n",
    "                    J_norm = J_val[0] - J_opt_shifted[0]\n",
    "                    append_result(results_opt, 'Alg1', seed, alpha, lamb, beta, str(p), str(np.hstack(theta_p).round(2)), J_norm, -1)\n",
    "                \n",
    "                if 'roar' in params['algs']:\n",
    "                    if beta in np.arange(0., 1.1, 0.2):\n",
    "                        x, _ = roar_recourse.get_recourse(x_0, beta=beta, theta_p=theta_p)\n",
    "                        J_val = J.eval(x, weights_s, bias_s)\n",
    "                        J_norm = J_val[0] - J_opt_shifted[0]\n",
    "                        append_result(results_roar, 'ROAR', seed, alpha, lamb, beta, str(p), str(np.hstack(theta_p).round(2)), J_norm, -1)\n",
    "    \n",
    "    df_results = pd.DataFrame()\n",
    "    if 'alg1' in params['algs']:\n",
    "        df_opt = pd.DataFrame(results_opt)\n",
    "        if params['save_history']:\n",
    "            print(f'[Alg1] Saving history for {dataset.name} run {seed}')\n",
    "            df_opt.to_pickle(f'../results/smoothness/history/{params[\"base_model\"]}_{dataset.name}_alg1_{seed}.pkl')\n",
    "        df_opt_agg = df_opt.groupby(['alg', 'p', 'beta'], as_index=False).mean(True)\n",
    "        if params['save_results']:\n",
    "            print(f'[Alg1] Saving results for {dataset.name} run {seed}')\n",
    "            df_opt_agg.to_pickle(f'../results/smoothness/output/{params[\"base_model\"]}_{dataset.name}_alg1_{seed}.pkl')\n",
    "        df_results = pd.concat((df_results, df_opt_agg))\n",
    "    \n",
    "    if 'roar' in params['algs']:\n",
    "        df_roar = pd.DataFrame(results_roar)\n",
    "        if params['save_history']:\n",
    "            print(f'[ROAR] Saving history for {dataset.name} run {seed}')\n",
    "            df_roar.to_pickle(f'../results/smoothness/history/{params[\"base_model\"]}_{dataset.name}_roar_{seed}.pkl')\n",
    "        df_roar_agg = df_roar.groupby(['alg', 'p', 'beta'], as_index=False).mean(True)\n",
    "        if params['save_results']:\n",
    "            print(f'[ROAR] Saving results for {dataset.name} run {seed}')\n",
    "            df_roar_agg.to_pickle(f'../results/smoothness/output/{params[\"base_model\"]}_{dataset.name}_roar_{seed}.pkl')\n",
    "        df_results = pd.concat((df_results, df_roar_agg))\n",
    "    \n",
    "    return df_results\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset: Dataset, params: dict, results: List):\n",
    "    alpha = params['alpha']\n",
    "    \n",
    "    for seed in params['seeds']:\n",
    "        (train_data, test_data), (train_data_shifted, test_data_shifted) = dataset.get_data(seed, shift=True)\n",
    "        X_train, y_train = train_data\n",
    "        X_test, y_test = test_data\n",
    "        X_train_shifted, y_train_shifted = train_data_shifted\n",
    "        X_test_shifted, y_test_shifted = test_data_shifted\n",
    "        \n",
    "        if params['base_model'] == 'nn':\n",
    "            base_model, shifted_model = NN(X_train.shape[1]), NN(X_train.shape[1])\n",
    "        else:\n",
    "            base_model, shifted_model = LR(), LR()\n",
    "            \n",
    "        base_model.train(X_train.values, y_train.values)\n",
    "        shifted_model.train(X_train_shifted.values, y_train_shifted.values)\n",
    "        \n",
    "        recourse_needed_X_train = recourse_needed(base_model.predict, X_train.values)\n",
    "        recourse_needed_X_test = recourse_needed(base_model.predict, X_test.values)\n",
    "        \n",
    "        weights_0, bias_0 = None, None\n",
    "        weights_s, bias_s = None, None\n",
    "        predictions = []\n",
    "        if params['base_model'] == 'lr':\n",
    "            weights_0 = base_model.model.coef_[0]\n",
    "            bias_0 = base_model.model.intercept_\n",
    "            theta_0 = np.hstack((weights_0, bias_0))\n",
    "            \n",
    "            weights_s = shifted_model.model.coef_[0]\n",
    "            bias_s = shifted_model.model.intercept_\n",
    "            theta_s = np.hstack((weights_s, bias_s))\n",
    "            predictions = generate_lr_smoothness_predictions(theta_0, theta_s, alpha)\n",
    "\n",
    "        lar_recourse = LARRecourse(weights=weights_0, bias=bias_0, alpha=alpha)\n",
    "        lar_recourse_shifted = LARRecourse(weights=weights_s, bias=bias_s, alpha=alpha)\n",
    "        roar_recourse = ROAR(weights=weights_0, bias=bias_0, alpha=alpha)\n",
    "        \n",
    "        params['lamb'] = lar_recourse.choose_lambda(recourse_needed_X_train, base_model.predict, X_train)\n",
    "        lar_recourse.lamb = params['lamb']\n",
    "        lar_recourse_shifted.lamb = params['lamb']\n",
    "        roar_recourse.lamb = params['lamb']\n",
    "        \n",
    "        # params['lamb_roar'] = roar_recourse.choose_lambda(recourse_needed_X_train, base_model.predict, X_train)\n",
    "        # roar_recourse.lamb = params['lamb_roar']\n",
    "        \n",
    "        df_result = recourse_runner(seed, recourse_needed_X_test, lar_recourse, lar_recourse_shifted, roar_recourse, base_model, shifted_model, X_train, params, dataset, predictions)\n",
    "        results.append(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sba data...\n",
      "Choosing lambda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lambda=0.1: 100%|██████████| 151/151 [00:00<00:00, 30027.49it/s]\n",
      "lambda=0.2: 100%|██████████| 151/151 [00:00<00:00, 30174.85it/s]\n",
      "lambda=0.3: 100%|██████████| 151/151 [00:00<00:00, 27444.64it/s]\n",
      "lambda=0.4: 100%|██████████| 151/151 [00:00<00:00, 27201.82it/s]\n",
      "lambda=0.5: 100%|██████████| 151/151 [00:00<00:00, 29286.04it/s]\n",
      "lambda=0.6: 100%|██████████| 151/151 [00:00<00:00, 27240.43it/s]\n",
      "lambda=0.7: 100%|██████████| 151/151 [00:00<00:00, 30265.69it/s]\n",
      "lambda=0.8: 100%|██████████| 151/151 [00:00<00:00, 28860.33it/s]\n",
      "lambda=0.9: 100%|██████████| 151/151 [00:00<00:00, 27463.68it/s]\n",
      "lambda=1.0: 100%|██████████| 151/151 [00:00<00:00, 28325.95it/s]\n",
      "Evaluating recourse | alpha=0.5; lambda=1.0:   0%|\u001b[38;2;0;145;255m          \u001b[0m| 0/38 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "d_results = {}\n",
    "params = {}\n",
    "params['alpha'] = 0.5\n",
    "params['lamb'] = None\n",
    "params['lamb_roar'] = None\n",
    "params['base_model'] = 'lr' # 'lr', 'nn\n",
    "params['seeds'] = range(4,5)\n",
    "params['algs'] = ['roar'] # 'alg1', 'roar\n",
    "params['save_results'] = True\n",
    "params['save_history'] = True\n",
    "params['save_final_results'] = False\n",
    "\n",
    "\n",
    "# datasets = [SyntheticDataset(), GermanDataset(), SBADataset()]\n",
    "datasets = [SBADataset()]\n",
    "for dataset in datasets:\n",
    "    results = []\n",
    "    \n",
    "    print(f'Running {dataset.name} data...')\n",
    "    run_experiment(dataset, params, results)\n",
    "    \n",
    "    d_results[dataset.name] = pd.concat(results)\n",
    "    if params['save_final_results']:\n",
    "        d_results[dataset.name].to_pickle(f'../results/smoothness/output/lr_{dataset.name}')\n",
    "    \n",
    "    print(f'Finished {dataset.name}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
